{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 추론 기반 기법과 신경망  \n",
    "-확률 기반으로 단어를 벡터로 표현하는것에는 한계가 있음.  \n",
    "-예를 들어 svd를 사용한다고 하더라고 100만 단어 종류가 있을 경우 계산량이 상당히 많이 들어감.(n^3)  \n",
    "-따라서 미니배치 방식의 딥러닝 방식이라면 100만 단어라도 충분히 처리가 가능함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론기반 기법?  \n",
    "-추론 기반 기법이란, 간단히 말해서 문맥을 통해서 어떤 단어가 와야할지를 예측하는 방법."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 - 단어\n",
    "-신경망에서는 단어 문자 그대로를 처리할수 없으므로 단어를 처리할수 있는 형태를 취해야함.  \n",
    "-그게 흔히 많이 사용하는 방법인 one-hot 표현 방식임.  \n",
    "-c는 임의의 원핫으로 표현한 벡터, 여기에 가중치(웨이트)W를 곱해서 단일 히든 레이어를 구성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84753331  0.31089763  0.78646923]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([1,0,0,0,0,0,0])\n",
    "W = np.random.randn(7,3)\n",
    "h = np.matmul(c, W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 사용했던 레이어를 다시 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grad = [np.zeros_like(W)]\n",
    "        self.x =None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        \n",
    "        ## numpy deep copy.\n",
    "        # 완전히 같지는 않지만 C언어에서 포인터의 느낌. 생략기호를 넣으면 값이 완전히 바뀌고,\n",
    "        # 일반적으로 하듯이 대입을 하면 메모리를 가르키는 위치만 변경.\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02087071 -0.98057648 -1.63714674]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "c = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "W = np.random.randn(7,3)\n",
    "layer = MatMul(W)\n",
    "\n",
    "h = layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBow(continuous bag of words)형태의 word2vec  \n",
    "-두개의 입력층을 가지고 하나의 은닉층을 거쳐 출력을 하는 형태로 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 먼저는 인퍼런스 부분만 구현해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.60864335  2.10805258 -1.60691172 -3.35894147 -4.6980309   3.07601584\n",
      " -1.28832815]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "#두 개의 임의의 입력 벡터 선언\n",
    "c0 = np.array([1, 0, 0, 0, 0, 0, 0])\n",
    "c1 = np.array([0, 0, 1, 0, 0, 0, 0])\n",
    "\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-여기서 생각할만한 것은 word2vec을 하는 목표가 단어의 분산표현임.  \n",
    "-하지만 W_in W_out둘다 단어의 분산표현이라고 할수 있을텐데 어떤것을 사용할지가 관건. 주로 W_in을 사용하는게 대세라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제는 역전파를 적용해서 학습까지...  \n",
    "-학습에는 cross entropy & softmax를 사용합니다.  \n",
    "-먼저는 앞에서 구현했던 전처리 코드를 다시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "text = 'You say goodbye and i say hello'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-이 코드를 사용하면 문자을 단어로 쪼개고, 각 단어들에게 번호, 라벨을 부여합니다.  \n",
    "-다음으로는 cbow의 학습 데이터 형태를 위해 데이터 전처리 준비를 합니다.  \n",
    "-cbow는 문맥이라고 하는 즉, 맞추고자 하는 단어를 기준으로 양옆과 같은 주위에 있는 단어를 입력으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]]\n",
      "[1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "print(contexts)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-이제 데이터를 사용하기 위해서 라벨 값들을 원핫 백터로 표현해주는 함수를 구현하자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None\n",
    "        self.t = None  \n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "\n",
    "class BasicCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        \n",
    "        W_in = 0.01 * np.random.randn(V,H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H,V).astype('f')\n",
    "        \n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "        layers = [self.in_layer0,\n",
    "                  self.in_layer1,\n",
    "                  self.out_layer\n",
    "                 ]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        \n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        self.word_vecs = W_in   \n",
    "        \n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:,0])\n",
    "        h1 = self.in_layer1.forward(contexts[:,1])\n",
    "        h = 0.5*(h0+h1)\n",
    "        score = self.out_layer(h)\n",
    "        loss = self.loss_layer(score, target)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *=0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이제 마지막으로 이 코드들을 가지고 학습이 되도록 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') \n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지금까지 구현한 CBOW모델을 개선해보자.  \n",
    "-지금까지 구현한 모델은 단점이 있다.  \n",
    "1.은닉층에 들어갈떄, 지금은 단어의 수가 적지만 100만 단위가 되면 연산량이 많이 소모되고 W행렬의 크기또한 커진다. 중간의 은닉층에서의 역할이 인덱싱임을 생각하고 곱하기 연산을 줄여보자.  \n",
    "2.현재는 최종 출력을 softmax를 사용하지만 위와 마찬가지로 단위가 상승되면 그 연산도 만만치 않다. 따라서 새로운 방법인 네거티브 샘플링을 사용하자.  \n",
    "-네거티브 샘플링은 간단하게 말해서 학습을 할 때 맞는 정답만 훈련하는게 아니라 틀린 단어도 같이 학습을 하고 나온 스코어들을 다 더한후에 로스를 계산한다.  \n",
    "-여기에는 기본적으로 각각을 시그모이드 함수로 변경해주고 one vs all 느낌으로 관점을 바꾸어서 계산을 하게된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
