{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완전계층구조 - forward만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "x = np.random.randn(10,2)\n",
    "\n",
    "h = np.matmul(x,W1) +b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비선형을 위한 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(2,4)\n",
    "b1 = np.random.randn(4)\n",
    "x = np.random.randn(10,2)\n",
    "\n",
    "w2 = np.random.randn(4,3)\n",
    "b2 = np.random.rand(3)\n",
    "\n",
    "h = np.matmul(x,W1) +b1\n",
    "a = sigmoid(h)\n",
    "h2 = np.matmul(a,w2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 나온 결과는 단순한 점수score, 클래스별로 확률을 적용하기 위해서는 출력층에 softmax등을 사용해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클래스로 구현하기  \n",
    "각 클래스에는 forward와 backward로 구성.  \n",
    "초기화로 입력 받는 파라미터들을 params로 선언."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구현을 위한 클래스 정의  \n",
    "시그모이드와 fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    def forward(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.params=[W,b]\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x,W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, ouput_size):\n",
    "        \n",
    "        W1 = np.random.randn(input_size, hidden_size)\n",
    "        b1 = np.random.randn(hidden_size)\n",
    "        W2 = np.random.randn(hidden_size, ouput_size)\n",
    "        b2 = np.random.randn(ouput_size)\n",
    "        \n",
    "        self.layers=[\n",
    "            Affine(W1,b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2,b2)\n",
    "        ]\n",
    "        \n",
    "        self.params = []\n",
    "        for layer in self.params:\n",
    "            self.params += layers.params\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.random.randn(10,2)\n",
    "model = TwoLayerNet(2, 4, 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습을 위한 loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat 노드  & Sum 노드\n",
    "쉽게 생각하면 한줄의 백터가 있을 때, 해당 백터(로우)를 여러개 반복해서 붙이는것.  \n",
    "역전파의 계산에서는 해당 노드는 sum을 해준다.  \n",
    "두 노드는 서로 반대로 작동한다고 생각.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D, N = 8, 7\n",
    "\n",
    "x = np.random.randn(1,D)\n",
    "y = np.repeat(x, N, axis=0)\n",
    "\n",
    "dy = np.random.randn(N,D)\n",
    "dx = np.sum(dy,axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## matmul 노드  \n",
    "dL/dX = dL/dY * W^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grad = [np.zeros_like(W)]\n",
    "        self.x =None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.matmul(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dW = np.matmul(self.x.T, dout)\n",
    "        \n",
    "        ## numpy deep copy.\n",
    "        # 완전히 같지는 않지만 C언어에서 포인터의 느낌. 생략기호를 넣으면 값이 완전히 바뀌고,\n",
    "        # 일반적으로 하듯이 대입을 하면 메모리를 가르키는 위치만 변경.\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid 노드  \n",
    "위에서 구현한것을 다시 사용.  \n",
    "시그모이드는 미분하면 y(1-x)가 됨.\n",
    "역전파를 전달할떄는 이 수치에 전달받은 미분값을 곱한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.grads = []\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out  = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine 계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.params=[W,b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.matmul(x,W) + b\n",
    "        return out\n",
    "    \n",
    "    def bacward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.matmul(dout, W.T)\n",
    "        dw = np.matmul(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        \n",
    "        self.grads[0][...] = dw\n",
    "        self.grads[1][...] = db\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax with loss 계층  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, parmas, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 문제 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 계산을 고속화하기 위한 방법들  \n",
    "1. 비트 정밀도  \n",
    "-넘파이는 float64를 기본으로 사용하고 있음. 이를 32비트로 할경우 계산이나 메모리측면에서 이득을 볼수 있음.  \n",
    "\n",
    "2. 쿠파이  \n",
    "-GPU좋은것은 요즘 많이 다 아실것이라 생각. \n",
    "-numpy를 GPU용으로 사용할수 있는게 cupy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
